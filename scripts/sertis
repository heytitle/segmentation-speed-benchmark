#! /usr/bin/env python3

import sys 
sys.path.append('thai-word-segmentation')

import tensorflow as tf
import pandas as pd
import numpy as np
import os

import utils

from tqdm import tqdm

from thainlplib import ThaiWordSegmentLabeller
from thainlplib.model import ThaiWordSegmentationModel

SAVED_MODEL_PATH = './thai-word-segmentation/saved_model'
TMP_FILE = '.tmptfrecord'

with tf.device('/device:GPU:0'):

    session = tf.Session(config=tf.ConfigProto(log_device_placement=True))

    model = tf.saved_model.loader.load(session, [tf.saved_model.tag_constants.SERVING], SAVED_MODEL_PATH)
    signature = model.signature_def[tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
    graph = tf.get_default_graph()

    g_inputs = graph.get_tensor_by_name(signature.inputs['inputs'].name)
    g_lengths = graph.get_tensor_by_name(signature.inputs['lengths'].name)
    g_training = graph.get_tensor_by_name(signature.inputs['training'].name)
    g_outputs = graph.get_tensor_by_name(signature.outputs['outputs'].name)

def nonzero(a):
    return [i for i, e in enumerate(a) if e != 0]

def split(s, indices):
    return [s[i:j] for i,j in zip(indices, indices[1:]+[None])]

def tokeniser(sentence):

    with tf.device('/device:GPU:0'):
        inputs = [ThaiWordSegmentLabeller.get_input_labels(sentence)]
        lengths = [len(sentence)]

        y = session.run(g_outputs, feed_dict={
            g_inputs: inputs,
            g_lengths: lengths,
            g_training: False
        })

        indices = nonzero(y) if y[0] == 1 else [0] + nonzero(y)

    return split(sentence, indices)

filename = sys.argv[1]
total = utils.count_lines(filename)
with open(filename, "r") as fin, open("dummy_output", "w") as fout, \
    utils.Timer("sertis", filename) as t:
    for l in tqdm(fin, total=total):
        l = l.strip()
        fout.write("%s\n" % "|".join(tokeniser(l)))